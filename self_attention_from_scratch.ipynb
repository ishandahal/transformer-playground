{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c195ed33-3da6-4135-83c8-3c419cfb5c3f",
   "metadata": {},
   "source": [
    "### Self-attention\n",
    "Notebook to see lower level workings of self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f41b50d5-b70a-4379-b271-2a1ace3d8e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590fe542-4e22-4286-95c8-7b5f0b199f0a",
   "metadata": {},
   "source": [
    "The idea of self attention (in context of text processing) is: given an input sequence create a vector representation of each token. The vector is going to contain information about the token in context with the rest of the tokens in the sequence.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da7b1527-25fa-45c6-8895-ac7f4304fe2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Life': 0, 'dessert': 1, 'eat': 2, 'first': 3, 'is': 4, 'short': 5}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'Life is short, eat dessert first'\n",
    "\n",
    "sentence = sentence.replace(',', '').split()\n",
    "\n",
    "vocab = {word: idx for idx, word in enumerate(sorted(sentence))}\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca1e202b-7a47-4148-8209-af8fe781640e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 4, 5, 2, 1, 3])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = torch.tensor([vocab[word] for word in sentence])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7c4318a-3de1-4a94-8f44-a0a438f09a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "# Embeddings\n",
    "emb_dim = 16\n",
    "emb = nn.Embedding(len(vocab), emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "294a0d73-9d60-41fe-a049-cf63d6c70e68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 16])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pass tokens through embedding layer\n",
    "inputs = emb(tokens).detach()\n",
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b3a4d8e-99e2-4fac-aaad-6dd405ee74c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3374, -0.1778, -0.3035, -0.5880,  0.3486,  0.6603, -0.2196, -0.3792,\n",
       "          0.7671, -1.1925,  0.6984, -1.4097,  0.1794,  1.8951,  0.4954,  0.2692],\n",
       "        [ 0.5146,  0.9938, -0.2587, -1.0826, -0.0444,  1.6236, -2.3229,  1.0878,\n",
       "          0.6716,  0.6933, -0.9487, -0.0765, -0.1526,  0.1167,  0.4403, -1.4465],\n",
       "        [ 0.2553, -0.5496,  1.0042,  0.8272, -0.3948,  0.4892, -0.2168, -1.7472,\n",
       "         -1.6025, -1.0764,  0.9031, -0.7218, -0.5951, -0.7112,  0.6230, -1.3729],\n",
       "        [-1.3250,  0.1784, -2.1338,  1.0524, -0.3885, -0.9343, -0.4991, -1.0867,\n",
       "          0.8805,  1.5542,  0.6266, -0.1755,  0.0983, -0.0935,  0.2662, -0.5850],\n",
       "        [-0.0770, -1.0205, -0.1690,  0.9178,  1.5810,  1.3010,  1.2753, -0.2010,\n",
       "          0.4965, -1.5723,  0.9666, -1.1481, -1.1589,  0.3255, -0.6315, -2.8400],\n",
       "        [ 0.8768,  1.6221, -1.4779,  1.1331, -1.2203,  1.3139,  1.0533,  0.1388,\n",
       "          2.2473, -0.8036, -0.2808,  0.7697, -0.6596, -0.7979,  0.1838,  0.2293]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49cc364-b806-4383-9fc3-9f6f158ed1d8",
   "metadata": {},
   "source": [
    "We have arbitrarily chosen embedding dimension of 24. To calculate self-attention for token 2 we will need the query vector of token 2 and key and value vectors for all tokens of the sequence. The vectors are created by matrix multiplying embedding vectors with Query, Key and Value matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2853e9f-3588-489e-b30d-7972767a4505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.5146,  0.9938, -0.2587, -1.0826, -0.0444,  1.6236, -2.3229,  1.0878,\n",
      "         0.6716,  0.6933, -0.9487, -0.0765, -0.1526,  0.1167,  0.4403, -1.4465])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([16])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_2 = inputs[1]\n",
    "print(token_2)\n",
    "token_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "27c8fc3c-2569-4cca-bd35-327d7dc86f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "# Dimensions for query, key and value matrices\n",
    "d_q, d_k, d_v = 24, 24, 28\n",
    "W_query = nn.Parameter(torch.rand(d_q, emb_dim))\n",
    "W_key = nn.Parameter(torch.rand(d_k, emb_dim))\n",
    "W_value = nn.Parameter(torch.rand(d_v, emb_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "68c1ac1a-8915-407f-bdd2-f1e7efd9ee64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([24, 16]), torch.Size([24, 16]), torch.Size([28, 16]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_query.shape, W_key.shape, W_value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "26fd707d-b20d-4ef7-9fdb-c04c11adb856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([24]), torch.Size([24]), torch.Size([28]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_2 = token_2.matmul(W_query.T) # Query vector for token 2\n",
    "key_2 = token_2.matmul(W_key.T) # Key vector for token 2\n",
    "value_2 = token_2.matmul(W_value.T)\n",
    "query_2.size(), key_2.size(), value_2.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fa8481b9-3f31-46f0-938f-74876b9a730d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 24]), torch.Size([6, 28]))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating Key, Value vector for all tokens in the sequence\n",
    "# keys = inputs.matmul(W_key.T)\n",
    "keys = W_key.matmul(inputs.T).T\n",
    "values = W_value.matmul(inputs.T).T\n",
    "keys.shape, values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d59393f9-afc9-476e-af3e-43bd19731c98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(11.1466, grad_fn=<DotBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "omega_24 = query_2.dot(keys[4])\n",
    "omega_24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ff7e1856-ed25-47f0-8e3c-923a9a7bee6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8.5808, -7.6597,  3.2558,  1.0395, 11.1466, -0.4800],\n",
       "       grad_fn=<SqueezeBackward4>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "omega_2 = query_2.matmul(keys.T) \n",
    "omega_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0cd5e7b1-01d0-4034-9f9d-13cca7dbe713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2912, 0.0106, 0.0982, 0.0625, 0.4917, 0.0458],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights2 = F.softmax((omega_2 / d_q**0.5), dim=0)\n",
    "attention_weights2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8d13b308-dbbe-4391-ba04-664b2d0291b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7392,  1.6789,  2.7937,  1.4616, -0.8727,  1.1213, -0.8441,  0.2829,\n",
       "          1.7343,  1.6112,  2.1563,  1.1398,  1.6928,  1.5737,  1.7709,  0.9618,\n",
       "          1.3077,  0.2716,  0.3070,  0.3427,  2.4012,  1.9869, -1.1107,  0.6782,\n",
       "         -0.2181,  0.8178,  0.5018,  0.9887],\n",
       "        [ 0.0166, -0.0809,  1.2402,  1.2786,  1.6755,  0.5242,  0.5165,  0.2638,\n",
       "          0.1946,  0.1296, -0.2176, -1.2548, -0.9272, -1.3402, -0.4107, -0.0859,\n",
       "          1.0926,  0.4078, -0.6770,  0.1110, -1.1055,  0.3156, -0.3169,  0.7937,\n",
       "         -1.1166,  3.0497, -0.2863,  1.5513],\n",
       "        [-4.1774, -1.6440, -1.9643, -1.6642, -1.0216, -5.0441, -1.4350, -3.0582,\n",
       "         -1.3735, -1.0167, -0.9397, -2.5408, -2.1351, -1.8701, -1.9994, -3.7609,\n",
       "         -3.8755, -3.1365, -2.1639, -3.0949, -3.7118, -1.8682, -1.8869, -1.7023,\n",
       "         -1.4043, -4.1602, -3.5326, -1.8202],\n",
       "        [ 0.5068, -1.0825, -2.4869, -0.3825,  0.3522, -3.0291, -1.0645, -0.9245,\n",
       "         -3.0223, -0.6932, -2.1795,  0.1399,  0.0171, -2.8164, -2.1397,  0.7122,\n",
       "         -0.7365, -1.5728, -2.6054, -1.3145, -0.8618, -0.5164, -1.8432, -3.0082,\n",
       "         -0.7213, -1.2709,  0.4588, -1.0818],\n",
       "        [-3.1399, -0.6158,  1.3958, -0.7103, -0.8951, -1.7725, -0.2955, -2.7453,\n",
       "          0.7757,  1.4735,  0.0459, -1.9004, -0.6969,  1.2049, -0.9530, -4.2561,\n",
       "          0.8399, -3.7423, -1.1625, -2.9441,  0.7369, -1.5681, -1.7600, -0.3928,\n",
       "         -1.6623, -1.0396,  0.1901,  2.8688],\n",
       "        [ 2.3501,  1.2960,  2.2324,  2.1957,  2.3762,  1.8197,  2.2329,  3.4829,\n",
       "         -1.9674,  3.0705,  0.6728,  3.1772,  2.7996,  0.7759,  2.7167,  2.6194,\n",
       "          0.1099,  0.9618,  1.1149,  3.5639,  3.5327,  2.4810,  2.8085,  2.3073,\n",
       "          2.6020,  4.4131,  3.1466,  5.2343]], grad_fn=<PermuteBackward0>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vector2 = attention_weights2.matmul(values)\n",
    "contex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c6f780-14c3-40ff-86f9-6fc63a752b13",
   "metadata": {},
   "source": [
    "We are going to use a single sentence to implement self-attention. We will use the individual words in the sentence as our vocabulary for simplicity. We will assign a number to each unique word in the vocabulary which will the token for the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25a5fb84-8924-4106-af88-55e7ae575ab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Life': 0, 'dessert': 1, 'eat': 2, 'first': 3, 'is': 4, 'short': 5}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'Life is short, eat dessert first'\n",
    "sentence = sentence.replace(',', '')\n",
    "\n",
    "vocab = {w:i for i,w in enumerate(sorted(sentence.split()))}\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "930d5536-4437-4cdc-be13-16a493d9b2e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 4, 5, 2, 1, 3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert words in sentence to tokens\n",
    "sentence_token = torch.tensor([vocab[word] for word in sentence.split()])\n",
    "sentence_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17a87ec-d1da-4395-9e41-a6a642383297",
   "metadata": {},
   "source": [
    "We will pass the tokens through an embedding layer. Embeddings are vectors with floating point numbers. The vector values after successful round of model training captures semantic meanings of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "345018a4-7b93-4340-9582-e94da3d50348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 16])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dim = 16\n",
    "\n",
    "emb = nn.Embedding(len(vocab), dim)\n",
    "\n",
    "# Passing sentence through embedding layer\n",
    "sentence_emb = emb(sentence_token)\n",
    "sentence_emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929a69f3-5235-4ed5-bbf1-cb4a65e2d894",
   "metadata": {},
   "source": [
    "The idea of self-attention is to capture the representation a word/token in relationship with all the other tokens in the sequence/input. To capture a vector representation second word in our sequence we calculate a query vector for the second word. We also calculate key and value vector for all the words in the sequence. These vectors are computed by Query, Key and Value matrices; values of which learned during training. Dot product between query vector for second word and all the other key vector creates an un-normalized score vector. This vector is divided by the square root of the number of dimensions of key vector. This is done so that the values do not exhibit numerical instability and helps convergence during training. The score vector goes through softmax to return a vector of probability distribution. This normalized score vector does a dot product with the value vectors to form self-attention score for the second word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc9101c0-78a1-43ca-ba1a-aa7b067f09a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([24, 16]), torch.Size([24, 16]), torch.Size([28, 16]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "q_d, k_d, v_d = 24, 24, 28\n",
    "W_query = nn.Parameter(torch.rand(q_d, dim))\n",
    "W_key = nn.Parameter(torch.rand(k_d, dim))\n",
    "W_value = nn.Parameter(torch.rand(v_d, dim))\n",
    "W_query.shape, W_key.shape, W_value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad0102c9-f07f-4ca5-b41f-95f621672269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_2 = sentence_emb[1]\n",
    "query_2 = W_query.matmul(token_2) # Query vector for token 2\n",
    "query_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31868a35-10e3-48eb-960a-991056da1234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 24]), torch.Size([6, 28]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = W_key.matmul(sentence_emb.T).T # Key vector for all tokens\n",
    "values = W_value.matmul(sentence_emb.T).T # Value vector for all tokens\n",
    "keys.shape, values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f6d7297-27a5-4442-a12d-a4382689bc20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(11.1466, grad_fn=<DotBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "omega24 = query_2.dot(keys[4]) # un-normalized score between second and fifth token\n",
    "omega24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bb0ded10-6ec4-40d5-b663-cd42f009c99a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8.5808, -7.6597,  3.2558,  1.0395, 11.1466, -0.4800],\n",
       "       grad_fn=<SqueezeBackward4>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "omega2 = query_2.matmul(keys.T) # un-normalized score second and all tokens\n",
    "omega2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "862eb46a-1903-46df-8898-84da3d2ee2bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2912, 0.0106, 0.0982, 0.0625, 0.4917, 0.0458],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights2 = F.softmax((omega2 / (k_d**0.5)), dim=0) # normalized attention scores\n",
    "attention_weights2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "067b8bf2-57a9-4783-a302-6511daa05046",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.5993,  0.0156,  1.2670,  0.0032, -0.6460, -1.1407, -0.4908, -1.4632,\n",
       "         0.4747,  1.1926,  0.4506, -0.7110,  0.0602,  0.7125, -0.1628, -2.0184,\n",
       "         0.3838, -2.1188, -0.8136, -1.5694,  0.7934, -0.2911, -1.3640, -0.2366,\n",
       "        -0.9564, -0.5265,  0.0624,  1.7084], grad_fn=<SqueezeBackward4>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# context_2 = (values.T).matmul(attention_weights2)\n",
    "context_2 = attention_weights2.matmul(values)\n",
    "context_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5ac58104-0b00-4808-9c77-2c9dccd742ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 24])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = W_query.matmul(sentence_emb.T).T # Query vectors for all tokens\n",
    "queries.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ea97deb1-5ae5-4db1-a80b-85a4a27cdab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  16.4255,    8.1306,  -24.5414,  -19.6606,   -9.5164,   19.2777],\n",
       "        [   8.5808,   -7.6597,    3.2558,    1.0395,   11.1466,   -0.4800],\n",
       "        [ -39.2836,   -1.5165,  145.4604,   74.2561,   58.8008, -141.6884],\n",
       "        [  -5.2174,   -4.6914,   74.9203,   30.6947,   35.7423,  -73.7312],\n",
       "        [ -21.6148,   10.6362,   65.4889,   39.2832,   21.8496,  -80.2922],\n",
       "        [  40.0110,   -8.6863, -129.7707,  -64.2901,  -39.9965,  102.5285]],\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "omega = queries.matmul(keys.T) # Un-normalized attention score \n",
    "omega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "be155ffd-2547-44c9-893f-48ebcd23576e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.3559e-01, 6.1726e-02, 7.8361e-05, 2.1222e-04, 1.6829e-03, 6.0071e-01],\n",
       "        [2.9123e-01, 1.0581e-02, 9.8213e-02, 6.2474e-02, 4.9169e-01, 4.5814e-02],\n",
       "        [4.1922e-17, 9.3433e-14, 1.0000e+00, 4.8723e-07, 2.0779e-08, 3.5016e-26],\n",
       "        [7.8632e-08, 8.7544e-08, 9.9954e-01, 1.2001e-04, 3.3626e-04, 6.6351e-14],\n",
       "        [1.8886e-08, 1.3652e-05, 9.9512e-01, 4.7287e-03, 1.3467e-04, 1.1868e-13],\n",
       "        [2.8696e-06, 1.3829e-10, 2.5508e-21, 1.6275e-15, 2.3183e-13, 1.0000e+00]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights = F.softmax((omega / (k_d**0.5)), dim=1) \n",
    "attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2d3ea6f2-d8eb-4445-9b18-4f60870bf0d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 28])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = attention_weights.matmul(values)\n",
    "context.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13e098e-2089-4c92-a904-7f4a8849532b",
   "metadata": {},
   "source": [
    "We have created a context vector for each token in the sequence. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439aec4a-fc6e-4e5c-9072-722ee0ca008e",
   "metadata": {},
   "source": [
    "### Multi-headed attention\n",
    "For multi-headed attention we will be doing the same computation we did above to create vector representations of each token weighted by the attention scores of each token. We will be using different Query, Key and Value matrices to create the initial vectors. The idea is to be able to capture different relationships betweens tokens with different attention weighted value vectors. Since the values of the matrices are updated during training; they will help capture dependencies between tokens in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "883a9c8f-78c9-474b-8bee-305901759a86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 24, 16]), torch.Size([3, 24, 16]), torch.Size([3, 28, 16]))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = 3\n",
    "multiheaded_w_query = nn.Parameter(torch.rand(h, q_d, dim))\n",
    "multiheaded_w_key = nn.Parameter(torch.rand(h, k_d, dim))\n",
    "multiheaded_w_value = nn.Parameter(torch.rand(h, v_d, dim))\n",
    "multiheaded_w_query.shape, multiheaded_w_key.shape, multiheaded_w_value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "29c870b7-1e95-4aea-b151-a2a54023a124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 24]), torch.Size([3, 24]), torch.Size([3, 28]))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiheaded_query_2 = multiheaded_w_query.matmul(token_2) # Three heads will create 3 three queries \n",
    "multiheaded_key_2 = multiheaded_w_key.matmul(token_2)\n",
    "multiheaded_value_2 = multiheaded_w_value.matmul(token_2)\n",
    "multiheaded_query_2.shape, multiheaded_key_2.shape, multiheaded_value_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9c20250c-b7cd-4fd5-9214-18c482b8d2b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 6, 16])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_inputs = sentence_emb.repeat(3, 1, 1) # Stacking inputs since we have multiple weight matrices \n",
    "stacked_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7bed5ac3-2783-43ca-867c-bdd678f2cbb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 24, 6]), torch.Size([3, 24, 6]), torch.Size([3, 28, 6]))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_headed_queries = multiheaded_w_query.bmm(stacked_inputs.permute(0, 2, 1))\n",
    "multi_headed_keys = multiheaded_w_key.bmm(stacked_inputs.permute(0, 2, 1))\n",
    "multi_headed_values = multiheaded_w_value.bmm(stacked_inputs.permute(0, 2, 1))\n",
    "multi_headed_queries.shape, multi_headed_keys.shape, multi_headed_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e2c77961-6765-449a-b421-72bc6155f2ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 6, 6])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_headed_scores = multi_headed_queries.permute(0, 2, 1).bmm(multi_headed_keys) # Attention scores (un-normalized)\n",
    "multi_headed_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b69317df-80fd-44dc-b46c-13dc5e739046",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 6, 6])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_headed_attention = F.softmax((multi_headed_scores / (q_d**0.5)), dim=-1) # Normalized attention scores \n",
    "multi_headed_attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e39ec4b9-99eb-433a-8fef-a08cb9a10fac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 6, 28])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_headed_context = multi_headed_attention.bmm(multi_headed_values.permute(0, 2, 1))\n",
    "multi_headed_context.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3506ddc1-2da0-4497-9de1-978a67ade287",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
