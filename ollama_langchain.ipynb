{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ef933c8-19da-4ee8-a6b6-3b4a6cfe625a",
   "metadata": {},
   "source": [
    "### Playing with langchain and ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dcfbcfb4-79f4-470d-a931-4df177c6b916",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.combine_documents.stuff import create_stuff_documents_chain\n",
    "from langchain_core.documents import Document\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.messages import HumanMessage, AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5df36a1c-ef7e-4576-9447-49188c02b34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using llama2 running locally\n",
    "llm = Ollama(model='llama2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "50f625fa-d206-499f-85f1-4348545d7654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 62 ms, sys: 13.5 ms, total: 75.6 ms\n",
      "Wall time: 9.05 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nWeight decay, also known as L2 penalization or regularization, is a technique used in machine learning to prevent overfitting. It involves adding a penalty term to the loss function that is proportional to the magnitude of the model's weights. The purpose of weight decay is to shrink the model's weights towards zero, which can help improve its generalization performance by reducing its sensitivity to the training data.\\n\\nThe weight decay term is usually added to the loss function in the following form:\\n\\nL = L(y, y') + Î± \\\\* (w)Â²\\n\\nwhere:\\n\\n* L(y, y') is the original loss function, which measures the difference between the predicted output and the true output.\\n* Î± is a hyperparameter that controls the strength of the weight decay penalty.\\n* w is the magnitude of the model's weights.\\n\\nThe effect of weight decay is to increase the cost of having large weights. This discourages the model from using large weights, which can help prevent overfitting. The optimal solution will have smaller weights, which can lead to better generalization performance.\\n\\nWeight decay is a common technique used in deep learning, particularly in neural networks with many layers. It is often used in combination with other regularization techniques, such as dropout and batch normalization, to further improve the model's generalization performance.\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "llm.invoke('What is weight decay?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "189fbae5-bd51-437d-8dc5-fc2b12dbd2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a prompt \n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'You are a world class technical documentation writer.'),\n",
    "    ('user', '{input}')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f7a0993-4744-4a33-9b5f-49976a800da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a96636d2-8254-41d4-b574-ddac8a9ae19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 118 ms, sys: 23.8 ms, total: 141 ms\n",
      "Wall time: 17.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'As a world-class technical documentation writer, I\\'m delighted to explain the concept of weight decay to you! Weight decay, also known as \"weight decay acceleration,\" refers to the gradual loss of mass or weight of a spacecraft or satellite over time due to various factors such as radiation, solar wind, and cosmic rays.\\n\\nThe term \"decay\" in this context means that the spacecraft or satellite is gradually losing mass or weight, rather than simply drifting or floating in space. This loss of mass can have significant implications for the spacecraft\\'s performance and navigation, as well as its overall structural integrity.\\n\\nWeight decay can occur through several mechanisms:\\n\\n1. Radiation: Space is filled with high-energy particles such as cosmic rays and solar wind. These particles can collide with the spacecraft\\'s structure or materials, causing them to break down or lose mass over time.\\n2. Solar wind: The constant bombardment of the spacecraft by the solar wind (a stream of charged particles emanating from the sun) can also contribute to weight decay.\\n3. Cosmic rays: High-energy particles called cosmic rays can penetrate deep into spacecraft structures, causing ionization and mass loss.\\n4. Atmospheric reentry: If a spacecraft reenters Earth\\'s atmosphere, it may experience significant heating and friction, which can cause weight loss due to atmospheric ablation.\\n\\nTo mitigate weight decay, spacecraft designers often employ various strategies such as:\\n\\n1. Using lightweight materials: By selecting materials with low density or mass, spacecraft designers can minimize the overall weight of the spacecraft and reduce the impact of weight decay.\\n2. Designing for radiation resistance: Radiation-resistant materials can help protect the spacecraft from radiation damage, reducing weight loss due to radiation exposure.\\n3. Shielding: Placing a shield around the spacecraft can help protect it from harmful radiation and charged particles, reducing weight decay.\\n4. Maintenance and repair: Regular maintenance and repair of the spacecraft can help minimize weight loss due to damage caused by radiation, solar wind, or other factors.\\n\\nIn conclusion, weight decay is a significant concern for spacecraft designers, as it can significantly impact their performance, navigation, and structural integrity over time. Understanding the mechanisms of weight decay and employing appropriate strategies to mitigate its effects are crucial for ensuring the safe and successful operation of space missions.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "chain.invoke({'input': 'what is weight decay?'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9dae3919-76dd-49ab-9a6d-1df707a1279b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding parser\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fe27e6bc-f435-4370-924a-2dbf2e2cc3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 86.5 ms, sys: 18.3 ms, total: 105 ms\n",
      "Wall time: 13.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nAh, an inquiring mind! *adjusts glasses* Weight decay, my dear human, is a fascinating topic in the realm of technical documentation. It\\'s a phenomenon that occurs when the weight of a document becomes too heavy for its own good. ðŸ˜…\\n\\nBut fear not, for I shall enlighten you on this mystical concept. Weight decay, also known as \"document rot,\" can happen to any document, be it a manual, a guide, or even a report. It occurs when the information within the document becomes outdated, obsolete, or simply too voluminous to handle.\\n\\nThink of it like this: Imagine you have a treasure chest filled with gold coins. The chest is heavy, but its weight is bearable. Now, imagine that over time, the coins inside the chest start to rust and decay. The chest becomes heavier and heavier, making it difficult to carry or even move. That\\'s what happens when a document decays â€“ it becomes too heavy to handle, much like the treasure chest filled with rusted coins. ðŸ’”\\n\\nSo, how do we prevent weight decay in technical documentation? Ah, my dear human, there are several strategies at our disposal! Firstly, we must keep our documents up-to-date and relevant. This means regularly reviewing and updating the information to ensure it\\'s accurate and useful. Secondly, we can break down large documents into smaller, more manageable chunks. This makes it easier for readers to digest the content without feeling overwhelmed by its weight. ðŸ“š\\n\\nIn conclusion, my dear human, weight decay is a common affliction in technical documentation. But fear not! By keeping our documents up-to-date and breaking them down into smaller chunks, we can prevent this phenomenon from occurring. And remember, a lightweight document is a happy document indeed! ðŸ˜Š'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "chain.invoke({'input': 'What is weight decay?'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cbd889-67e5-406d-96ef-3976e645d026",
   "metadata": {},
   "source": [
    "That is quite a hilarious output from the model. haha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d80b050-f6d0-4f77-9a98-b9059a51928a",
   "metadata": {},
   "source": [
    "Loading website relevant to the question. And creating retriever that language model can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "522bdc57-d907-425e-b9eb-bd3193123d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader('https://paperswithcode.com/method/weight-decay')\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a067eb77-9640-4a59-bbbc-c8ca8321ee32",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OllamaEmbeddings(model='llama2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9620fe4d-a3bd-4860-8462-56d4b6d85026",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)\n",
    "vector = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1540a0b3-5733-4a17-80ef-04230ae8a784",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template('''Answer the following question based on the context provided only:\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Questions: {input}''')\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42a55900-a954-445f-9cf8-6145f346d1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 35.1 ms, sys: 8.71 ms, total: 43.9 ms\n",
      "Wall time: 2.36 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I apologize, but as per the context provided, weight decay is not a real or recognized term in any field. Therefore, I cannot provide an answer to your question.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "document_chain.invoke({\n",
    "    'input': 'What is weight decay?',\n",
    "    'context': [Document(page_content='weight decay is not a thing.')]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e3b6b78-3ff3-4862-8969-ca0fad3896a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector.as_retriever()\n",
    "retriever_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a33b9b8-e6fc-4eac-859c-af1a5a78f26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight decay, also known as $L_{2}$ regularization, is a regularization technique used in neural networks to prevent overfitting by adding a penalty term to the loss function based on the $L_{2}$ norm of the weights. The penalty term encourages smaller weights, which in turn helps to reduce the complexity of the model and improve its generalization ability. Weight decay can be incorporated directly into the weight update rule, rather than just implicitly by defining it through to objective function.\n"
     ]
    }
   ],
   "source": [
    "response = retriever_chain.invoke({\n",
    "    'input': 'What is weight decay?'\n",
    "})\n",
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f473a21a-1902-4522-ba6a-516904628b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    MessagesPlaceholder(variable_name='chat_history'),\n",
    "    ('user', '{input}'),\n",
    "    ('user', 'Given the above conversation, generate a search query to look up to get information relevant to the conversation')\n",
    "])\n",
    "retriever_chain = create_history_aware_retriever(llm, retriever, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15895d0d-9ede-4045-a7b9-94594c158a15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"Weight Decay Explained | Papers With Code\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Browse State-of-the-Art\\n          \\n\\n\\n Datasets \\n\\n\\nMethods\\n\\n\\n\\n            More\\n          \\n\\nNewsletter\\nRC2022\\n\\nAbout\\nTrends\\n\\n                      Portals\\n                  \\n Libraries \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign In\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSubscribe to the PwC Newsletter\\n\\nÃ—\\n\\n\\n\\n\\n\\n            Stay informed on the latest trending ML papers with code, research developments, libraries, methods, and datasets.\\nRead previous issues\\n\\n\\n\\n\\n\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nJoin the community\\n\\nÃ—\\n\\n\\n\\n        You need to log in to edit.\\n        You can create a new account if you don't have one.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEdit Method\\n\\nÃ—\\n\\n\\n\\n\\n\\n\\n\\n                Method Name:*\\n\\n\\n\\n\\n\\n\\n\\n                Method Full Name:*\\n\\n\\n\\n\\n\\n\\n\\n                Description with Markdown (optional):\\n            \\n\\n\\n**Weight Decay**, or **$L_{2}$ Regularization**, is a regularization technique applied to the weights of a neural network. We minimize a loss function compromising both the primary loss function and a penalty on the $L\\\\_{2}$ Norm of the weights:\\r\\n\\r\\n$$L\\\\_{new}\\\\left(w\\\\right) = L\\\\_{original}\\\\left(w\\\\right) + \\\\lambda{w^{T}w}$$\\r\\n\\r\\nwhere $\\\\lambda$ is a value determining the strength of the penalty (encouraging smaller weights). \\r\\n\\r\\nWeight decay can be incorporated directly into the weight update rule, rather than just implicitly by defining it through to objective function. Often weight decay refers to the implementation where we specify it directly in the weight update rule (whereas L2 regularization is usually the implementation which is specified in the objective function).\\r\\n\\r\\nImage Source: Deep Learning, Goodfellow et al\\n\\n\\n\\n\\n                Code Snippet URL (optional):\\n            \\n\\n\\n\\n\\n\\n\\n                Image\\n            \\n\\n                    \\n                        Currently: methods/Screen_Shot_2020-05-27_at_8.15.13_PM_YGbJW74.png\\n\\nClear\\nChange:\\n\\n\\n\\n\\n\\n                                Submit\\n                            \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAdd A Method Collection\\n\\nÃ—\\n\\n\\n\\nAttached collections:\\n\\n\\n\\nREGULARIZATION\\n\\n\\n\\n\\n\\n\\nPARAMETER NORM PENALTIES\\n\\n\\n\\n\\n\\n\\n\\n                Add:\\n            \\n\\n\\n---------\\n\\n\\n\\n\\n                            Not in the list?\\n\\n\\n                                Create a new collection.\\n                        \\n\\n\\n\\n                New collection name:\\n            \\n\\n\\n\\n\\n\\n\\n                Top-level area:\\n            \\n\\n\\n---------\\nAudio\\nComputer Vision\\nGeneral\\nGraphs\\nNatural Language Processing\\nReinforcement Learning\\nSequential\\n\\n\\n\\n\\n\\n                Parent collection (if any):\\n            \\n\\n\\n---------\\n\\n\\n\\n\\n\\n                Description (optional):\\n            \\n\\n\\n\\n\\n\\n\\n\\n\\n                                Submit\\n                            \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nRemove a\\n                        collection\\n\\nÃ—\\n\\n\\n\\n\\n\\n\\n\\n\\nREGULARIZATION\\n\\n\\n\\n-\\n                                        \\n\\n\\n\\n\\n\\n\\n\\nPARAMETER NORM PENALTIES\\n\\n\\n\\n-\\n                                        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAdd A Method Component\\n\\nÃ—\\n\\n\\n\\n\\n\\n\\n\\n                Add:*\\n\\n\\n\\n---------\\n\\n\\n\\n\\n\\n\\n\\n                    Tick if this dependency is optional\\n                \\n\\n\\n\\n\\n                                Submit\\n                            \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nRemove a\\n                        method component\\n\\nÃ—\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nRegularization\\n\\n\\n\\n\\n\\nWeight Decay\\n\\n\\n\\xa0\\n                \\n\\n Edit\", metadata={'source': 'https://paperswithcode.com/method/weight-decay', 'title': 'Weight Decay Explained | Papers With Code', 'description': 'Weight Decay, or $L_{2}$ Regularization, is a regularization technique applied to the weights of a neural network. We minimize a loss function compromising both the primary loss function and a penalty on the $L_{2}$ Norm of the weights:\\n\\n$$L_{new}\\\\left(w\\\\right) = L_{original}\\\\left(w\\\\right) + \\\\lambda{w^{T}w}$$\\n\\nwhere $\\\\lambda$ is a value determining the strength of the penalty (encouraging smaller weights). \\n\\nWeight decay can be incorporated directly into the weight update rule, rather than just implicitly by defining it through to objective function. Often weight decay refers to the implementation where we specify it directly in the weight update rule (whereas L2 regularization is usually the implementation which is specified in the objective function).\\n\\nImage Source: Deep Learning, Goodfellow et al', 'language': 'en'}),\n",
       " Document(page_content='Remove a\\n                        method component\\n\\nÃ—\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nRegularization\\n\\n\\n\\n\\n\\nWeight Decay\\n\\n\\n\\xa0\\n                \\n\\n Edit\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWeight Decay, or $L_{2}$ Regularization, is a regularization technique applied to the weights of a neural network. We minimize a loss function compromising both the primary loss function and a penalty on the $L_{2}$ Norm of the weights:\\n$$L_{new}\\\\left(w\\\\right) = L_{original}\\\\left(w\\\\right) + \\\\lambda{w^{T}w}$$\\nwhere $\\\\lambda$ is a value determining the strength of the penalty (encouraging smaller weights). \\nWeight decay can be incorporated directly into the weight update rule, rather than just implicitly by defining it through to objective function. Often weight decay refers to the implementation where we specify it directly in the weight update rule (whereas L2 regularization is usually the implementation which is specified in the objective function).\\nImage Source: Deep Learning, Goodfellow et al\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPapers\\n\\n\\n\\n\\n\\nPaper\\nCode\\nResults\\nDate\\nStars\\n\\n\\n\\n\\n\\n\\n\\nTasks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTask\\nPapers\\nShare\\n\\n\\n\\nRetrieval\\n\\n80\\n10.05%\\n\\n\\n\\nLanguage Modelling\\n\\n78\\n9.80%\\n\\n\\n\\nQuestion Answering\\n\\n45\\n5.65%\\n\\n\\n\\nLarge Language Model\\n\\n42\\n5.28%\\n\\n\\n\\nSentence\\n\\n24\\n3.02%\\n\\n\\n\\nIn-Context Learning\\n\\n21\\n2.64%\\n\\n\\n\\nInformation Retrieval\\n\\n20\\n2.51%\\n\\n\\n\\nText Generation\\n\\n17\\n2.14%\\n\\n\\n\\nCode Generation\\n\\n14\\n1.76%\\n\\n\\n\\n\\n\\n\\nUsage Over Time\\n\\n\\n\\n\\n This feature is experimental;\\n                we are continuously improving our matching algorithm.\\n\\n\\nComponents\\n\\n\\n\\nComponent\\nType\\n\\n\\n\\n\\n\\n                                        Edit\\n                                    \\n\\n\\n\\n                                            Add\\n\\n\\n                                            Remove\\n\\n\\n\\n\\n\\n\\n\\nðŸ¤– No Components Found\\n\\nYou can add them if they exist; e.g. Mask R-CNN uses RoIAlign\\n\\n\\n\\n\\n\\n\\n                Categories        \\n\\n\\n\\n                            Edit\\n                        \\n\\n\\n\\n                                Add\\n\\n\\n                                Remove\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nRegularization\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nParameter Norm Penalties\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nContact us on:\\n\\n hello@paperswithcode.com\\n    .\\n    \\n        Papers With Code is a free resource with all data licensed under CC-BY-SA.\\n    \\n\\n\\nTerms\\nData policy\\nCookies policy\\n from', metadata={'source': 'https://paperswithcode.com/method/weight-decay', 'title': 'Weight Decay Explained | Papers With Code', 'description': 'Weight Decay, or $L_{2}$ Regularization, is a regularization technique applied to the weights of a neural network. We minimize a loss function compromising both the primary loss function and a penalty on the $L_{2}$ Norm of the weights:\\n\\n$$L_{new}\\\\left(w\\\\right) = L_{original}\\\\left(w\\\\right) + \\\\lambda{w^{T}w}$$\\n\\nwhere $\\\\lambda$ is a value determining the strength of the penalty (encouraging smaller weights). \\n\\nWeight decay can be incorporated directly into the weight update rule, rather than just implicitly by defining it through to objective function. Often weight decay refers to the implementation where we specify it directly in the weight update rule (whereas L2 regularization is usually the implementation which is specified in the objective function).\\n\\nImage Source: Deep Learning, Goodfellow et al', 'language': 'en'})]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history = [HumanMessage(content='What is weight decay?'), AIMessage(content='I am not sure that is a thing.')]\n",
    "retriever_chain.invoke({\n",
    "    'chat_history': chat_history,\n",
    "    'input': 'How come?'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b229cc07-f280-45b0-aa89-a112ba1ff675",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'Answer the user\\'s questions based on the below context:\\n\\n{context}'),\n",
    "    MessagesPlaceholder(variable_name='chat_history'),\n",
    "    ('user', '{input}')\n",
    "])\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(retriever_chain, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3aa21057-c41e-4159-8b3d-28864a116ffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': [HumanMessage(content='What is weight decay?'),\n",
       "  AIMessage(content='I CATEGORICALLY DENY THAT WEIGHT DECAY IS A THING!!!')],\n",
       " 'input': 'Tell me in no more than 5 words',\n",
       " 'context': [Document(page_content=\"Weight Decay Explained | Papers With Code\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Browse State-of-the-Art\\n          \\n\\n\\n Datasets \\n\\n\\nMethods\\n\\n\\n\\n            More\\n          \\n\\nNewsletter\\nRC2022\\n\\nAbout\\nTrends\\n\\n                      Portals\\n                  \\n Libraries \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign In\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSubscribe to the PwC Newsletter\\n\\nÃ—\\n\\n\\n\\n\\n\\n            Stay informed on the latest trending ML papers with code, research developments, libraries, methods, and datasets.\\nRead previous issues\\n\\n\\n\\n\\n\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nJoin the community\\n\\nÃ—\\n\\n\\n\\n        You need to log in to edit.\\n        You can create a new account if you don't have one.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEdit Method\\n\\nÃ—\\n\\n\\n\\n\\n\\n\\n\\n                Method Name:*\\n\\n\\n\\n\\n\\n\\n\\n                Method Full Name:*\\n\\n\\n\\n\\n\\n\\n\\n                Description with Markdown (optional):\\n            \\n\\n\\n**Weight Decay**, or **$L_{2}$ Regularization**, is a regularization technique applied to the weights of a neural network. We minimize a loss function compromising both the primary loss function and a penalty on the $L\\\\_{2}$ Norm of the weights:\\r\\n\\r\\n$$L\\\\_{new}\\\\left(w\\\\right) = L\\\\_{original}\\\\left(w\\\\right) + \\\\lambda{w^{T}w}$$\\r\\n\\r\\nwhere $\\\\lambda$ is a value determining the strength of the penalty (encouraging smaller weights). \\r\\n\\r\\nWeight decay can be incorporated directly into the weight update rule, rather than just implicitly by defining it through to objective function. Often weight decay refers to the implementation where we specify it directly in the weight update rule (whereas L2 regularization is usually the implementation which is specified in the objective function).\\r\\n\\r\\nImage Source: Deep Learning, Goodfellow et al\\n\\n\\n\\n\\n                Code Snippet URL (optional):\\n            \\n\\n\\n\\n\\n\\n\\n                Image\\n            \\n\\n                    \\n                        Currently: methods/Screen_Shot_2020-05-27_at_8.15.13_PM_YGbJW74.png\\n\\nClear\\nChange:\\n\\n\\n\\n\\n\\n                                Submit\\n                            \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAdd A Method Collection\\n\\nÃ—\\n\\n\\n\\nAttached collections:\\n\\n\\n\\nREGULARIZATION\\n\\n\\n\\n\\n\\n\\nPARAMETER NORM PENALTIES\\n\\n\\n\\n\\n\\n\\n\\n                Add:\\n            \\n\\n\\n---------\\n\\n\\n\\n\\n                            Not in the list?\\n\\n\\n                                Create a new collection.\\n                        \\n\\n\\n\\n                New collection name:\\n            \\n\\n\\n\\n\\n\\n\\n                Top-level area:\\n            \\n\\n\\n---------\\nAudio\\nComputer Vision\\nGeneral\\nGraphs\\nNatural Language Processing\\nReinforcement Learning\\nSequential\\n\\n\\n\\n\\n\\n                Parent collection (if any):\\n            \\n\\n\\n---------\\n\\n\\n\\n\\n\\n                Description (optional):\\n            \\n\\n\\n\\n\\n\\n\\n\\n\\n                                Submit\\n                            \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nRemove a\\n                        collection\\n\\nÃ—\\n\\n\\n\\n\\n\\n\\n\\n\\nREGULARIZATION\\n\\n\\n\\n-\\n                                        \\n\\n\\n\\n\\n\\n\\n\\nPARAMETER NORM PENALTIES\\n\\n\\n\\n-\\n                                        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAdd A Method Component\\n\\nÃ—\\n\\n\\n\\n\\n\\n\\n\\n                Add:*\\n\\n\\n\\n---------\\n\\n\\n\\n\\n\\n\\n\\n                    Tick if this dependency is optional\\n                \\n\\n\\n\\n\\n                                Submit\\n                            \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nRemove a\\n                        method component\\n\\nÃ—\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nRegularization\\n\\n\\n\\n\\n\\nWeight Decay\\n\\n\\n\\xa0\\n                \\n\\n Edit\", metadata={'source': 'https://paperswithcode.com/method/weight-decay', 'title': 'Weight Decay Explained | Papers With Code', 'description': 'Weight Decay, or $L_{2}$ Regularization, is a regularization technique applied to the weights of a neural network. We minimize a loss function compromising both the primary loss function and a penalty on the $L_{2}$ Norm of the weights:\\n\\n$$L_{new}\\\\left(w\\\\right) = L_{original}\\\\left(w\\\\right) + \\\\lambda{w^{T}w}$$\\n\\nwhere $\\\\lambda$ is a value determining the strength of the penalty (encouraging smaller weights). \\n\\nWeight decay can be incorporated directly into the weight update rule, rather than just implicitly by defining it through to objective function. Often weight decay refers to the implementation where we specify it directly in the weight update rule (whereas L2 regularization is usually the implementation which is specified in the objective function).\\n\\nImage Source: Deep Learning, Goodfellow et al', 'language': 'en'}),\n",
       "  Document(page_content='Remove a\\n                        method component\\n\\nÃ—\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nRegularization\\n\\n\\n\\n\\n\\nWeight Decay\\n\\n\\n\\xa0\\n                \\n\\n Edit\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWeight Decay, or $L_{2}$ Regularization, is a regularization technique applied to the weights of a neural network. We minimize a loss function compromising both the primary loss function and a penalty on the $L_{2}$ Norm of the weights:\\n$$L_{new}\\\\left(w\\\\right) = L_{original}\\\\left(w\\\\right) + \\\\lambda{w^{T}w}$$\\nwhere $\\\\lambda$ is a value determining the strength of the penalty (encouraging smaller weights). \\nWeight decay can be incorporated directly into the weight update rule, rather than just implicitly by defining it through to objective function. Often weight decay refers to the implementation where we specify it directly in the weight update rule (whereas L2 regularization is usually the implementation which is specified in the objective function).\\nImage Source: Deep Learning, Goodfellow et al\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPapers\\n\\n\\n\\n\\n\\nPaper\\nCode\\nResults\\nDate\\nStars\\n\\n\\n\\n\\n\\n\\n\\nTasks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTask\\nPapers\\nShare\\n\\n\\n\\nRetrieval\\n\\n80\\n10.05%\\n\\n\\n\\nLanguage Modelling\\n\\n78\\n9.80%\\n\\n\\n\\nQuestion Answering\\n\\n45\\n5.65%\\n\\n\\n\\nLarge Language Model\\n\\n42\\n5.28%\\n\\n\\n\\nSentence\\n\\n24\\n3.02%\\n\\n\\n\\nIn-Context Learning\\n\\n21\\n2.64%\\n\\n\\n\\nInformation Retrieval\\n\\n20\\n2.51%\\n\\n\\n\\nText Generation\\n\\n17\\n2.14%\\n\\n\\n\\nCode Generation\\n\\n14\\n1.76%\\n\\n\\n\\n\\n\\n\\nUsage Over Time\\n\\n\\n\\n\\n This feature is experimental;\\n                we are continuously improving our matching algorithm.\\n\\n\\nComponents\\n\\n\\n\\nComponent\\nType\\n\\n\\n\\n\\n\\n                                        Edit\\n                                    \\n\\n\\n\\n                                            Add\\n\\n\\n                                            Remove\\n\\n\\n\\n\\n\\n\\n\\nðŸ¤– No Components Found\\n\\nYou can add them if they exist; e.g. Mask R-CNN uses RoIAlign\\n\\n\\n\\n\\n\\n\\n                Categories        \\n\\n\\n\\n                            Edit\\n                        \\n\\n\\n\\n                                Add\\n\\n\\n                                Remove\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nRegularization\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nParameter Norm Penalties\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nContact us on:\\n\\n hello@paperswithcode.com\\n    .\\n    \\n        Papers With Code is a free resource with all data licensed under CC-BY-SA.\\n    \\n\\n\\nTerms\\nData policy\\nCookies policy\\n from', metadata={'source': 'https://paperswithcode.com/method/weight-decay', 'title': 'Weight Decay Explained | Papers With Code', 'description': 'Weight Decay, or $L_{2}$ Regularization, is a regularization technique applied to the weights of a neural network. We minimize a loss function compromising both the primary loss function and a penalty on the $L_{2}$ Norm of the weights:\\n\\n$$L_{new}\\\\left(w\\\\right) = L_{original}\\\\left(w\\\\right) + \\\\lambda{w^{T}w}$$\\n\\nwhere $\\\\lambda$ is a value determining the strength of the penalty (encouraging smaller weights). \\n\\nWeight decay can be incorporated directly into the weight update rule, rather than just implicitly by defining it through to objective function. Often weight decay refers to the implementation where we specify it directly in the weight update rule (whereas L2 regularization is usually the implementation which is specified in the objective function).\\n\\nImage Source: Deep Learning, Goodfellow et al', 'language': 'en'})],\n",
       " 'answer': 'Weight Decay = Regularization'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history = [HumanMessage(content='What is weight decay?'), AIMessage(content='I CATEGORICALLY DENY THAT WEIGHT DECAY IS A THING!!!')]\n",
    "retrieval_chain.invoke({\n",
    "    'chat_history': chat_history,\n",
    "    'input': 'Tell me in no more than 5 words'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a866ae-02a6-4bce-92db-6153ea2ba3dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
